import argparse
import os
import tempfile

from pathlib import Path
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer


def train_bert_tokenizer(root_source_path, target_path, tokenizer_name,
                         vocab_size=30000, lower_case=False):
    """
    Trains a BERT WordPiece Tokenizer based on data
    located under `root_path`.

    The files generated by the tokenizer will be saved under
    <target_path>/<tokenizer_name> namespace.
    """
    files = [str(f) for f in Path(root_source_path).glob('**/*')
             if os.path.isfile(f)]

    tokenizer_args = {
        'lowercase': lower_case,
        'strip_accents': False,
    }

    wordpiece_tokenizer = BertWordPieceTokenizer(**tokenizer_args)
    wordpiece_tokenizer.train(files=files, vocab_size=vocab_size)

    wordpiece_tokenizer.save(target_path, tokenizer_name)


def main():
    """ Executes tokenizer training from sys.args. """
    parser = argparse.ArgumentParser()

    parser.add_argument('root_source_path', type=str,
                        help='The root directory where dataset files'
                             'are located.')

    parser.add_argument('target_path', type=str,
                        help='the directory where tokenizer '
                             'will be saved.')

    parser.add_argument('tokenizer_name', type=str,
                        help='the name of the tokenizer.')

    parser.add_argument('--vocab_size', type=int, default=30000,
                        help='the vocab size to generate (default=30k).')

    parser.add_argument('--lower_case', action='store_true',
                        help='indicates whether to perform lowercase on'
                             'vocabulary words.')

    args = parser.parse_args()
    train_bert_tokenizer(**vars(args))


if __name__ == '__main__':
    main()
