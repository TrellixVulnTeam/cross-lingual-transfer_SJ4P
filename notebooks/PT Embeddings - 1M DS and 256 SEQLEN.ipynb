{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PT Embeddings - 1M DS and 256 SEQLEN.ipynb","provenance":[{"file_id":"1GnLVFjfP6NZDsod82_neQA3hVy_wkoVM","timestamp":1590588420525}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNj+IMDoujKIWV002S/hR4u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"058a2ccd4d06419b95abcb491980b6c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9f161b5f5d6c407c848fbdfa920525f0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5683803001c048f5925ee28b478d51d6","IPY_MODEL_34207058aae64be48a2ebeba77560a24"]}},"9f161b5f5d6c407c848fbdfa920525f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5683803001c048f5925ee28b478d51d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_afd2da1d30624b969347f38ae0848bc8","_dom_classes":[],"description":"Epoch: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc1d8aee54024d27856a06cccdb31671"}},"34207058aae64be48a2ebeba77560a24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e2119ec60bae46448864b68cf04a9959","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [5:38:46&lt;00:00, 20326.28s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_200be64561ab46999b9d48fbddfd7439"}},"afd2da1d30624b969347f38ae0848bc8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dc1d8aee54024d27856a06cccdb31671":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2119ec60bae46448864b68cf04a9959":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"200be64561ab46999b9d48fbddfd7439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d81d1d728e8c4e4ca1e2bc4304752c9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a5c7a435fd444900ae99c22cbd5298aa","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6786928a34ba4bb093802be5fd5e7aee","IPY_MODEL_a8324958164240b9bc60c09d0f095566"]}},"a5c7a435fd444900ae99c22cbd5298aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6786928a34ba4bb093802be5fd5e7aee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_16d29d8bca2b47d99e613d1a989da10e","_dom_classes":[],"description":"Iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":31250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":31250,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a718121fec4c4f1c9489d274333bade8"}},"a8324958164240b9bc60c09d0f095566":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_22b66aa47076450493699f60b4624355","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 31250/31250 [5:38:46&lt;00:00,  1.54it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b2f7fde127f94db6b8ee41611e238b0b"}},"16d29d8bca2b47d99e613d1a989da10e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a718121fec4c4f1c9489d274333bade8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"22b66aa47076450493699f60b4624355":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b2f7fde127f94db6b8ee41611e238b0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"irQIbmGLargJ","colab_type":"text"},"source":["# PT Word Embeddings Training\n","\n","In this notebook, I'll pre-train the Word Embeddingd from BERT Base Cased, while all other parameters.\n","\n","This approach is the first one suggested by Artetxe, Ruder and Yogatama (2020). The rationale behind this is to test the hypothesis that BERT can learning more abstract concepts, beyond only language information. So, if we only change its Word Embeddings (the lexical part), it could still have a strong performance on downstream tasks for other languages that it was not fine tuned to.\n"]},{"cell_type":"markdown","metadata":{"id":"IaxtaxqwPOhI","colab_type":"text"},"source":["## Parameters\n","\n","Artetxe, Ruder and Yogatama (2020) pre-train a BERT from scracth in an English Corpus and, then, freeze all parameters other than the Word Embeddings, so they can train those embeddings in a language specific corpus (Portuguese, in my case).\n","\n","This work will take advantage of the already pre-trained BERT in English from Huggingface transformer's library. So, in order to better reproduce the experiment made by Artetxe, Ruder and Yogatama, it is very import that the parameters are relatively aligned.\n","\n","For this experiment, I'll use the following configuration, in comparision to their work:\n","\n","\n","| Parameter  |      Artetxe, Ruder and Yogatama (2020)       |  Mine (pt Embeddings) |\n","|------------|:-------------:|:------:|\n","| model architectute   |  BERT Base | BERT Base |\n","| model implementation | Their own, based on BERT's paper | **Huggingface transformer's library** |\n","| Corpus   |    Wikipedia dumps (WikiExtractor)   |   Wikipedia dumps (WikiExtractor) |\n","| data preprocessing   | no normalization, no lowercase |    no normalization, no lowercase |\n","| vocabulary   | disjoint, training on language-specific corpus |    disjoint, training on language-specific corpus |\n","| vocab size | 32k | **28,996** |\n","| special token ids (compared to English model) | all aligned | all aligned |\n","| pre-training objective | MLM and NLP | **MLM only** |\n","| trainable part | Word Embeddings | Word Embeddings |\n","| optimizer | LAMB | **AdamW, with LR decay** |\n","| sequence length | 512 | **256** |\n","| transfer training steps | 250k | **31,250** |\n","\n","The highlighted differences are explained as follows:\n","\n","1.   **model implementation**. I use the Huggingface transformer's library, which already contains a pre-trained BERT Base Cased, similar to the training made by Artetxe, Ruder and Yogatama (2020). This choice changes some parameters because I want my Word Embeddings training to be aligned with the library's pre-training, as the authors do with the English and Language Specific models.\n","\n","2.   **vocab size**. This parameter is aligned with the vocab size of the pre-trained English model from Huggingface's library.\n","\n","3. **pre-training objective**. I could not find the objective used in HUggingface's pre-training, but when downloading the model (**bert-base-cased**) and checking the architecture, one can see **BertForMaskedLM**, which would be the model for pre-training BERT in MLM objective (their is also a **BertForNextSentencePrediction** and **BertForPretraining** classes, but none of them appears in the configuration of the pre-trained model). So I assume this objective as the only one to be made in my experiment.\n","\n","4. **optimizer**. We use AdamW with learning rate decay as the default optimizer of Huggingface's **Trainer** API. The original experiment use an optimizer called LAMB, which was proposed by You et al. (2019).\n","\n","4. **sequence length**. I truncate the sequences to 256, instead of 512 as in the original experiment. This is done to fit CUDA memory in Google's Colab platform. Yet to be investigated a better way to do this.\n","\n","5. **transfer training steps**. I pre-train the embeddings using a very small subset of the entire pt Wiki Corpus (which contains about 6 million sentences). I use only 1M sentences from that corpus, with 32 as batch size, leaving the training phase with 31,250 steps. This is done to keep the pre-training phase shorter in time (~5h, using 1x NVIDIA Tesla P100 in Google Colab).\n","\n","I aim to make more pre-training with the Portuguese corpus, making the parameters more aligned with the original experiment in the future. \n","\n","Another important note is that this notebook is based on the one provided by [Huggingface on how to pre-train a language model from scratch](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=QDNgPls7_l13). I try to use this approach, so other paramaters and implementations are better aligned to theirs, since the **Trainer** API is an attempt to make the training of their models reproducible by others.\n","\n","\n","> For more information on WikiExtractor tool and how I prepared the data, refer to [the tool's Github](https://github.com/attardi/wikiextractor) and [my Colab Notebook](https://colab.research.google.com/drive/1aQHa7Hp5-HZFwrsBqxUDnQXbqxbgNC0r).\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3FHm94KuPGsX","colab_type":"text"},"source":["## Preparation\n","\n","We install libraries, mount drive and copy the preprocessed sentences to local disk."]},{"cell_type":"code","metadata":{"id":"8no2GpivavYC","colab_type":"code","colab":{}},"source":["!pip install transformers --quiet\n","!pip install pytorch_lightning --quiet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CR59RLaa1Ot","colab_type":"code","outputId":"8f7ba688-cc95-4e3a-b6e7-8e394914680e","executionInfo":{"status":"ok","timestamp":1590589058168,"user_tz":180,"elapsed":7328,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2PSSQc9Fa_2h","colab_type":"code","outputId":"65729574-fe7e-460e-b1af-598e08d6314b","executionInfo":{"status":"ok","timestamp":1590589058168,"user_tz":180,"elapsed":7315,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["%%time\n","\n","import os\n","import shutil\n","\n","if not os.path.exists('/content/dataset.pkl'):\n","    shutil.copy('/content/drive/My Drive/PF13/text/preprocessed.pkl',\n","                '/content/dataset.pkl')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["CPU times: user 30 µs, sys: 7 µs, total: 37 µs\n","Wall time: 59.1 µs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j4iRkH5og0sB","colab_type":"text"},"source":["## Tokenizer\n","\n","I'll first load my pre-trained tokenizer (`bert-base-cased-pt`) (See [this notebook for reference](https://colab.research.google.com/drive/1aQHa7Hp5-HZFwrsBqxUDnQXbqxbgNC0r))."]},{"cell_type":"code","metadata":{"id":"WDkgC4PkdpmD","colab_type":"code","colab":{}},"source":["from transformers import  BertTokenizer\n","\n","\n","TOKENIZER_VOCAB = '/content/drive/My Drive/PF13/bert-base-cased-pt-vocab.txt'\n","\n","tokenizer = BertTokenizer(TOKENIZER_VOCAB,\n","                          do_lower_case=False,\n","                          model_max_length=128)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u725YcYmhoFI","colab_type":"text"},"source":["## Dataset\n","\n","I create my own Dataset (instead of using the transformers `LineByLineTextDataset`) so I can load files in a lazy way."]},{"cell_type":"code","metadata":{"id":"oJhOp5qB7S1y","colab_type":"code","colab":{}},"source":["import pickle\n","\n","\n","with open('/content/dataset.pkl', 'rb') as f:\n","    all_sentences = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpmUWGpPh36A","colab_type":"code","colab":{}},"source":["import torch\n","\n","from torch.utils.data import Dataset\n","\n","\n","class WikiPtDataset(Dataset):\n","    def __init__(self, sentences, tokenizer, max_length=128):\n","        self.examples = sentences\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","    \n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        encoded = self.tokenizer.batch_encode_plus(\n","            [self.examples[idx]],\n","            max_length=self.max_length)\n","        \n","        return torch.tensor(encoded['input_ids'][0],\n","                            dtype=torch.long)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYnkUDX7b4Gg","colab_type":"code","outputId":"0dba3231-22ce-453d-b6ce-6a69ce393719","executionInfo":{"status":"ok","timestamp":1590589093044,"user_tz":180,"elapsed":42146,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["%%time\n","\n","dataset = WikiPtDataset(all_sentences, tokenizer)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n","Wall time: 10.3 µs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WegaBhUTfKI4","colab_type":"code","outputId":"5e2867e7-9302-41ab-c623-9861ea2bfd19","executionInfo":{"status":"ok","timestamp":1590589093045,"user_tz":180,"elapsed":42129,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["import random\n","\n","random_sample = random.randint(0, len(dataset))\n","\n","print('Dataset size=', len(dataset))\n","print('Raw sample=', len(dataset.examples[random_sample]), dataset.examples[random_sample])\n","print('Encoded sample=', dataset[random_sample][:10])\n","print('Decoded sample=', tokenizer.decode(dataset[random_sample]))\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Dataset size= 6180082\n","Raw sample= 387 Ao final da investigação, a FIFA decretou a vitória da seleção brasileira por 2 x 0, baniu Roberto Rojas, o técnico Orlando Aravena, o médico da seleção chilena Daniel Rodríguez e o dirigente Sergio Stoppel e suspendeu por 4 anos o capitão Fernando Astengo e a Federação Chilena de Futebol, o que acarretou na impossibilidade desta disputar as eliminatórias para a Copa do Mundo de 1994.\n","Encoded sample= tensor([  101,  3493,  2552,  1926,  7893,   117,   170,  7733, 27051,   170])\n","Decoded sample= [CLS] Ao final da investigação, a FIFA decretou a vitória da seleção brasileira por 2 x 0, baniu Roberto Rojas, o técnico Orlando Aravena, o médico da seleção chilena Daniel Rodríguez e o dirigente Sergio Stoppel e suspendeu por 4 anos o capitão Fernando Astengo e a Federação Chilena de Futebol, o que acarretou na impossibilidade desta disputar as eliminatórias para a Copa do Mundo de 1994. [SEP]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ij-R67B5ysHR","colab_type":"text"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"XTVP_cl7Ys0X","colab_type":"text"},"source":["### Training Environment\n","\n","For training, I'll use the GPU provided by Google Colab."]},{"cell_type":"code","metadata":{"id":"5cP7zyCBkbL9","colab_type":"code","outputId":"0329c8c4-a96a-4259-a79e-c4dfb1692ec7","executionInfo":{"status":"ok","timestamp":1590589094742,"user_tz":180,"elapsed":43811,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":318}},"source":["!nvidia-smi"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Wed May 27 14:18:13 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_pLtizI3Yv0O","colab_type":"text"},"source":["I also make sure the the GPU is available to PyTorch."]},{"cell_type":"code","metadata":{"id":"NC_f_Bc0yt5L","colab_type":"code","outputId":"221ca2d7-0a73-4646-9512-bebfe6c4f2cd","executionInfo":{"status":"ok","timestamp":1590589094743,"user_tz":180,"elapsed":43793,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["torch.cuda.is_available()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"-s8PThQIZGx9","colab_type":"text"},"source":["### Loading the BERT English Model\n","\n","As stated in [Parameters section](#parameters), I'll use Huggingface's pre-trained **bert-base-cased** model."]},{"cell_type":"code","metadata":{"id":"8ORvfaVyZKWO","colab_type":"code","colab":{}},"source":["from transformers import BertForMaskedLM\n","\n","\n","bert_base_cased = BertForMaskedLM.from_pretrained('bert-base-cased')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SslugESPY1HX","colab_type":"text"},"source":["### Freezing Parameters\n","\n","As in the original experiment, I'll freeze all parameters, but the Word Embeddings. This enables the pre-training of the lexical part of BERT in the Portugues Corpus."]},{"cell_type":"code","metadata":{"id":"ybO81-crQAbe","colab_type":"code","colab":{}},"source":["for param in bert_base_cased.parameters():\n","    param.requires_grad = False\n","\n","for param in bert_base_cased.get_input_embeddings().parameters():\n","    param.requires_grad = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JRgCiXjZnSc","colab_type":"text"},"source":["To test that all parameters the correct parameters are froze, I'll count the parameters that requires gradient and the ones from Word Embeddings to check that they match."]},{"cell_type":"code","metadata":{"id":"y6Tqlj131yQ4","colab_type":"code","outputId":"4911a7d9-0111-4588-b2c5-86fa243e0dd4","executionInfo":{"status":"ok","timestamp":1590589098996,"user_tz":180,"elapsed":48009,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sum([torch.tensor(x.size()).prod() for x in bert_base_cased.parameters() if x.requires_grad])"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(22268928)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"1GF6xTSj2Wo-","colab_type":"code","outputId":"a560c6f7-cdc8-4264-a5f1-1b19ca8c015b","executionInfo":{"status":"ok","timestamp":1590589098997,"user_tz":180,"elapsed":47990,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(768*bert_base_cased.config.vocab_size)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["22268928\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LEVyVP4lQaSg","colab_type":"text"},"source":["### Data Preparation\n","\n","I'll use a small subset of the entire Portuguese Corpus. This decision will save up some time during pre-training. That'd be nice to evaluate later the effectiveness of this simplification."]},{"cell_type":"code","metadata":{"id":"1QWE2RaAaQxh","colab_type":"code","colab":{}},"source":["sentences = all_sentences[:1000000]\n","train_dataset = WikiPtDataset(sentences, tokenizer, 256)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"28PmdwRbaSD7","colab_type":"text"},"source":["I'll also use the transfomers library's `DataCollatorForLanguageModeling` component to mask the sentences for training. This DataCollator is using during batch collation on PyTorch's `DataLoader`.\n","\n","For more informatio, checkout its source code [here](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py)."]},{"cell_type":"code","metadata":{"id":"C9aHh6sFQUvd","colab_type":"code","colab":{}},"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jmvI-40ua0Vx","colab_type":"text"},"source":["### Execute training\n","\n","For training, instead of using PyTorch Lightning (as in previous experiments), I'll use transformers' **Trainer** API, as in [this colab](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=QDNgPls7_l13). This is a very straightforward API for training, make explicit training parameters and also making this reproducible."]},{"cell_type":"code","metadata":{"id":"RxGh4DirBwLQ","colab_type":"code","colab":{}},"source":["from transformers import Trainer, TrainingArguments"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoIJGsHA3Jz1","colab_type":"code","colab":{}},"source":["training_args = TrainingArguments(\n","    output_dir=\"./bert-base-cased-ptemb-2\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_gpu_train_batch_size=32,\n","    save_steps=10_000,\n","    save_total_limit=2,\n",")\n","\n","trainer = Trainer(\n","    model=bert_base_cased,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    prediction_loss_only=True,\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5jZS1TG3Xo0","colab_type":"code","outputId":"cf6739f5-0e14-4605-8330-9959a2bd692d","executionInfo":{"status":"ok","timestamp":1590609608767,"user_tz":180,"elapsed":20327018,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["058a2ccd4d06419b95abcb491980b6c2","9f161b5f5d6c407c848fbdfa920525f0","5683803001c048f5925ee28b478d51d6","34207058aae64be48a2ebeba77560a24","afd2da1d30624b969347f38ae0848bc8","dc1d8aee54024d27856a06cccdb31671","e2119ec60bae46448864b68cf04a9959","200be64561ab46999b9d48fbddfd7439","d81d1d728e8c4e4ca1e2bc4304752c9d","a5c7a435fd444900ae99c22cbd5298aa","6786928a34ba4bb093802be5fd5e7aee","a8324958164240b9bc60c09d0f095566","16d29d8bca2b47d99e613d1a989da10e","a718121fec4c4f1c9489d274333bade8","22b66aa47076450493699f60b4624355","b2f7fde127f94db6b8ee41611e238b0b"]}},"source":["%%time\n","trainer.train()"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"058a2ccd4d06419b95abcb491980b6c2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d81d1d728e8c4e4ca1e2bc4304752c9d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=31250.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["{\"loss\": 7.833941082000733, \"learning_rate\": 4.92e-05, \"epoch\": 0.016, \"step\": 500}\n","{\"loss\": 7.1357386054992675, \"learning_rate\": 4.8400000000000004e-05, \"epoch\": 0.032, \"step\": 1000}\n","{\"loss\": 6.961196041107177, \"learning_rate\": 4.76e-05, \"epoch\": 0.048, \"step\": 1500}\n","{\"loss\": 6.844485320091247, \"learning_rate\": 4.6800000000000006e-05, \"epoch\": 0.064, \"step\": 2000}\n","{\"loss\": 6.747228501319885, \"learning_rate\": 4.600000000000001e-05, \"epoch\": 0.08, \"step\": 2500}\n","{\"loss\": 6.682852043151856, \"learning_rate\": 4.52e-05, \"epoch\": 0.096, \"step\": 3000}\n","{\"loss\": 6.588532162666321, \"learning_rate\": 4.44e-05, \"epoch\": 0.112, \"step\": 3500}\n","{\"loss\": 6.539473986625671, \"learning_rate\": 4.36e-05, \"epoch\": 0.128, \"step\": 4000}\n","{\"loss\": 6.462820938110352, \"learning_rate\": 4.2800000000000004e-05, \"epoch\": 0.144, \"step\": 4500}\n","{\"loss\": 6.406926966667175, \"learning_rate\": 4.2e-05, \"epoch\": 0.16, \"step\": 5000}\n","{\"loss\": 6.345725014686584, \"learning_rate\": 4.12e-05, \"epoch\": 0.176, \"step\": 5500}\n","{\"loss\": 6.301571597099304, \"learning_rate\": 4.0400000000000006e-05, \"epoch\": 0.192, \"step\": 6000}\n","{\"loss\": 6.243855212211609, \"learning_rate\": 3.960000000000001e-05, \"epoch\": 0.208, \"step\": 6500}\n","{\"loss\": 6.187169748306275, \"learning_rate\": 3.88e-05, \"epoch\": 0.224, \"step\": 7000}\n","{\"loss\": 6.145094234466553, \"learning_rate\": 3.8e-05, \"epoch\": 0.24, \"step\": 7500}\n","{\"loss\": 6.0930825653076175, \"learning_rate\": 3.72e-05, \"epoch\": 0.256, \"step\": 8000}\n","{\"loss\": 6.039981253623963, \"learning_rate\": 3.6400000000000004e-05, \"epoch\": 0.272, \"step\": 8500}\n","{\"loss\": 6.0063012428283695, \"learning_rate\": 3.56e-05, \"epoch\": 0.288, \"step\": 9000}\n","{\"loss\": 5.96797018623352, \"learning_rate\": 3.48e-05, \"epoch\": 0.304, \"step\": 9500}\n","{\"loss\": 5.943025405883789, \"learning_rate\": 3.4000000000000007e-05, \"epoch\": 0.32, \"step\": 10000}\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["{\"loss\": 5.897396026611328, \"learning_rate\": 3.32e-05, \"epoch\": 0.336, \"step\": 10500}\n","{\"loss\": 5.841297324180603, \"learning_rate\": 3.24e-05, \"epoch\": 0.352, \"step\": 11000}\n","{\"loss\": 5.827618744850159, \"learning_rate\": 3.16e-05, \"epoch\": 0.368, \"step\": 11500}\n","{\"loss\": 5.79259323310852, \"learning_rate\": 3.08e-05, \"epoch\": 0.384, \"step\": 12000}\n","{\"loss\": 5.753235119819641, \"learning_rate\": 3e-05, \"epoch\": 0.4, \"step\": 12500}\n","{\"loss\": 5.734581573486328, \"learning_rate\": 2.9199999999999998e-05, \"epoch\": 0.416, \"step\": 13000}\n","{\"loss\": 5.6961206426620485, \"learning_rate\": 2.84e-05, \"epoch\": 0.432, \"step\": 13500}\n","{\"loss\": 5.669636674880982, \"learning_rate\": 2.7600000000000003e-05, \"epoch\": 0.448, \"step\": 14000}\n","{\"loss\": 5.650984806060791, \"learning_rate\": 2.6800000000000004e-05, \"epoch\": 0.464, \"step\": 14500}\n","{\"loss\": 5.624766125679016, \"learning_rate\": 2.6000000000000002e-05, \"epoch\": 0.48, \"step\": 15000}\n","{\"loss\": 5.595482871055603, \"learning_rate\": 2.5200000000000003e-05, \"epoch\": 0.496, \"step\": 15500}\n","{\"loss\": 5.581985260009765, \"learning_rate\": 2.44e-05, \"epoch\": 0.512, \"step\": 16000}\n","{\"loss\": 5.55482991027832, \"learning_rate\": 2.36e-05, \"epoch\": 0.528, \"step\": 16500}\n","{\"loss\": 5.528442962646484, \"learning_rate\": 2.2800000000000002e-05, \"epoch\": 0.544, \"step\": 17000}\n","{\"loss\": 5.526202614784241, \"learning_rate\": 2.2000000000000003e-05, \"epoch\": 0.56, \"step\": 17500}\n","{\"loss\": 5.508343458175659, \"learning_rate\": 2.12e-05, \"epoch\": 0.576, \"step\": 18000}\n","{\"loss\": 5.491655488014221, \"learning_rate\": 2.04e-05, \"epoch\": 0.592, \"step\": 18500}\n","{\"loss\": 5.462012858390808, \"learning_rate\": 1.9600000000000002e-05, \"epoch\": 0.608, \"step\": 19000}\n","{\"loss\": 5.4703231525421145, \"learning_rate\": 1.88e-05, \"epoch\": 0.624, \"step\": 19500}\n","{\"loss\": 5.422406888961792, \"learning_rate\": 1.8e-05, \"epoch\": 0.64, \"step\": 20000}\n","{\"loss\": 5.409756859779358, \"learning_rate\": 1.7199999999999998e-05, \"epoch\": 0.656, \"step\": 20500}\n","{\"loss\": 5.410129541397095, \"learning_rate\": 1.6400000000000002e-05, \"epoch\": 0.672, \"step\": 21000}\n","{\"loss\": 5.39094137096405, \"learning_rate\": 1.56e-05, \"epoch\": 0.688, \"step\": 21500}\n","{\"loss\": 5.354259212493896, \"learning_rate\": 1.48e-05, \"epoch\": 0.704, \"step\": 22000}\n","{\"loss\": 5.351733510971069, \"learning_rate\": 1.4000000000000001e-05, \"epoch\": 0.72, \"step\": 22500}\n","{\"loss\": 5.346406350135803, \"learning_rate\": 1.32e-05, \"epoch\": 0.736, \"step\": 23000}\n","{\"loss\": 5.337649784088135, \"learning_rate\": 1.24e-05, \"epoch\": 0.752, \"step\": 23500}\n","{\"loss\": 5.338434123039246, \"learning_rate\": 1.16e-05, \"epoch\": 0.768, \"step\": 24000}\n","{\"loss\": 5.332663395881653, \"learning_rate\": 1.08e-05, \"epoch\": 0.784, \"step\": 24500}\n","{\"loss\": 5.314757569313049, \"learning_rate\": 1e-05, \"epoch\": 0.8, \"step\": 25000}\n","{\"loss\": 5.301315527915954, \"learning_rate\": 9.2e-06, \"epoch\": 0.816, \"step\": 25500}\n","{\"loss\": 5.305738132476806, \"learning_rate\": 8.400000000000001e-06, \"epoch\": 0.832, \"step\": 26000}\n","{\"loss\": 5.281515263557434, \"learning_rate\": 7.6e-06, \"epoch\": 0.848, \"step\": 26500}\n","{\"loss\": 5.286287523269653, \"learning_rate\": 6.800000000000001e-06, \"epoch\": 0.864, \"step\": 27000}\n","{\"loss\": 5.275417205810547, \"learning_rate\": 6e-06, \"epoch\": 0.88, \"step\": 27500}\n","{\"loss\": 5.284313331604004, \"learning_rate\": 5.2e-06, \"epoch\": 0.896, \"step\": 28000}\n","{\"loss\": 5.258116366386414, \"learning_rate\": 4.4e-06, \"epoch\": 0.912, \"step\": 28500}\n","{\"loss\": 5.267184432029724, \"learning_rate\": 3.6e-06, \"epoch\": 0.928, \"step\": 29000}\n","{\"loss\": 5.250520526885986, \"learning_rate\": 2.8000000000000003e-06, \"epoch\": 0.944, \"step\": 29500}\n","{\"loss\": 5.26103138256073, \"learning_rate\": 2.0000000000000003e-06, \"epoch\": 0.96, \"step\": 30000}\n","{\"loss\": 5.237220956802368, \"learning_rate\": 1.2000000000000002e-06, \"epoch\": 0.976, \"step\": 30500}\n","{\"loss\": 5.25185733127594, \"learning_rate\": 4.0000000000000003e-07, \"epoch\": 0.992, \"step\": 31000}\n","\n","\n","CPU times: user 3h 43min 15s, sys: 1h 54min 22s, total: 5h 37min 38s\n","Wall time: 5h 38min 46s\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=31250, training_loss=5.785344849456787)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"4sHLxfD6cYVX","colab_type":"text"},"source":["### Saving the Model and Embeddings\n","\n","I save the entire model using **Trainer**'s  save utility. I also save only the word embeddings from Pytorch **Module**."]},{"cell_type":"code","metadata":{"id":"btKCokZv3kbT","colab_type":"code","colab":{}},"source":["trainer.save_model(\"/content/drive/My Drive/PF13/bert-base-cased-ptemb-v2\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_bdwftmcpa2","colab_type":"code","colab":{}},"source":["emb = bert_base_cased.get_input_embeddings()\n","torch.save(emb.state_dict(), '/content/drive/My Drive/PF13/pt-embeddings-v2')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLYpfzsWnUiO","colab_type":"code","outputId":"c9e1b703-471d-4d4f-9f0c-b2cdb0edee89","executionInfo":{"status":"ok","timestamp":1590609713133,"user_tz":180,"elapsed":560,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["emb"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(28996, 768, padding_idx=0)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"Ra0bqRe1ctmW","colab_type":"text"},"source":["## Checking the Model\n","\n","I decided to do a minor check if the model is now able to perform the MLM in Portuguese. Remember that only the word embeddings were trained in Portuguese and the rest of the Transformer is still with its English trained weights."]},{"cell_type":"code","metadata":{"id":"F4Ffy3Kjk3Lq","colab_type":"code","colab":{}},"source":["from transformers import pipeline\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=\"/content/drive/My Drive/PF13/bert-base-cased-ptemb-v2\",\n","    tokenizer=tokenizer\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7yO2CgAlYIo","colab_type":"code","outputId":"2ca0096d-f498-46fb-c0bb-fd2419209b66","executionInfo":{"status":"ok","timestamp":1590609758720,"user_tz":180,"elapsed":695,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":283}},"source":["fill_mask('Passagem é um [MASK] do estado da Paraíba')"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.21943828463554382,\n","  'sequence': '[CLS] Passagem é um estado do estado da Paraíba [SEP]',\n","  'token': 2574},\n"," {'score': 0.08214069902896881,\n","  'sequence': '[CLS] Passagem é um município do estado da Paraíba [SEP]',\n","  'token': 2835},\n"," {'score': 0.031121809035539627,\n","  'sequence': '[CLS] Passagem é um jogador do estado da Paraíba [SEP]',\n","  'token': 3803},\n"," {'score': 0.030938852578401566,\n","  'sequence': '[CLS] Passagem é um centro do estado da Paraíba [SEP]',\n","  'token': 3635},\n"," {'score': 0.016195986419916153,\n","  'sequence': '[CLS] Passagem é um campo do estado da Paraíba [SEP]',\n","  'token': 3931}]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"cLxjcDIaOakK","colab_type":"text"},"source":["## References\n","\n","Artetxe, Mikel, Sebastian Ruder, and Dani Yogatama. \"On the cross-lingual transferability of monolingual representations.\" arXiv preprint arXiv:1910.11856 (2020).\n","\n","You, Yang, et al. \"Large batch optimization for deep learning: Training bert in 76 minutes.\" International Conference on Learning Representations. 2019."]},{"cell_type":"code","metadata":{"id":"zVNl9jo-OdCu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}