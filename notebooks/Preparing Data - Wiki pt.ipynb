{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preparing Data - Wiki pt.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPRE1guJQ/cupR4mmX1sOnI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"grl3WOZ2kzQQ","colab_type":"code","outputId":"54f4f2e8-5a05-4b97-bb0b-b5a9ff995909","executionInfo":{"status":"ok","timestamp":1590516107637,"user_tz":180,"elapsed":10415,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":585}},"source":["!pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n","\u001b[K     |████████████████████████████████| 665kB 2.8MB/s \n","\u001b[?25hCollecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 46.8MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 41.0MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 45.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=31086e4ecf746b099ede8338cdcacbad2e18b44a9a3f97497ec8ee36300bc26b\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bdXS0WHKiyOd","colab_type":"code","outputId":"f7cd5310-a4f1-4a30-d808-a8340b8bf8d3","executionInfo":{"status":"ok","timestamp":1590516129704,"user_tz":180,"elapsed":32465,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UwuWnV48nVzc","colab_type":"text"},"source":["## Process Dataset\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VMJ-cnXDI2NP","colab_type":"text"},"source":["### Pre processing\n","\n","The data extracted from WikiExtractor tool is saved in the following format:\n","\n","```xml\n","<doc id=\"1438711\" url=\"https://pt.wikipedia.org/wiki?curid=1438711\" title=\"Lista de rainhas de Aragão\">\n","Lista de rainhas de Aragão\n","\n","Esta é uma lista das mulheres que usaram o título de Rainha de Aragão, ...\n","\n","\"Consortes de Aragão e de Navarra\"\n","\n","A partir de 1516, a união dos reinos espanhóis passou ...\n","</doc>\n","\n","```\n","\n","We must remove the enclosing doc tags and remove empty lines, so they are not considered during training."]},{"cell_type":"code","metadata":{"id":"T3MDA7NBnYDa","colab_type":"code","colab":{}},"source":["import re\n","\n","\n","WIKI_DOC_REGEX = re.compile(r'<[\\/]{0,1}doc')\n","\n","\n","def pre_process(all_sentences):\n","    \"\"\" Remove empty sentences and doc declarations from Wikipedia \"\"\"\n","    return filter(lambda s: s and not WIKI_DOC_REGEX.match(s), all_sentences)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vfQ-zC5KkGw","colab_type":"code","outputId":"dabc270e-b0cd-4209-c473-dcc40e01c35b","executionInfo":{"status":"ok","timestamp":1590516129706,"user_tz":180,"elapsed":32444,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Some sanity check\n","\n","raw_sentences = [\n","    '<doc id=\"__id\" url=\"__url\" title=\"__title\">',\n","    'Some title',\n","    '',\n","    'some initial text',\n","    '',\n","    '</doc>']\n","\n","list(pre_process(raw_sentences))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Some title', 'some initial text']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Ta1fXPfnJpTj","colab_type":"text"},"source":["### Load and Cache\n","\n","After being extract by WikiExtractor, we have 1,680 files containing 6,180,082 sentences (after pre processing).\n","\n","This is a considerable ammount of files to be read, so we read them all, pre-process them and save a cache of the final list. This helps performing texts with the final list without touching every wiki file."]},{"cell_type":"code","metadata":{"id":"aWDq5zHHKaqB","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","\n","\n","RAW_DATA_ROOT   = '/content/drive/My Drive/PF13/text'\n","CACHED_PATH     = '/content/drive/My Drive/PF13/text/preprocessed.pkl'\n","\n","\n","def load_raw_sentences(path):\n","    \"\"\"\n","    Load all sentences in the Wiki extracted structure.\n","    The files are considered to be generated from WikiExtractor tool:\n","    - https://github.com/attardi/wikiextractor\n","    \"\"\"\n","    sentences = []\n","\n","    for root, _, files in os.walk(path):\n","        for file in files:\n","            with open(os.path.join(root, file)) as f:\n","                sentences_in_file = [line.strip() for line in f]\n","                sentences.extend(sentences_in_file)\n","\n","    return sentences\n","\n","\n","def load_sentences(cache_it=True):\n","    \"\"\"\n","    Load sentences from cache.\n","    If cache is not available, process raw sentencens and cache them.\n","\n","    Parameters:\n","    - cache_it (default True): indicates whether raw sentences should be cached.\n","    \"\"\"\n","    if os.path.exists(CACHED_PATH):\n","        with open(CACHED_PATH, 'rb') as pic:\n","            return pickle.load(pic)\n","\n","    all_sentences = list(pre_process(load_raw_sentences(RAW_DATA_ROOT)))\n","\n","    if cache_it is True:\n","        with open(CACHED_PATH, 'wb') as pic:\n","            pickle.dump(all_sentences, pic)\n","\n","    return all_sentences"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pibt5K1Ln3eQ","colab_type":"code","outputId":"1b337109-61d3-46b4-a703-10dd93a71959","executionInfo":{"status":"ok","timestamp":1590516147368,"user_tz":180,"elapsed":50084,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["all_sentences = load_sentences()\n","\n","print('Loaded:', len(all_sentences))\n","print('Example:', all_sentences[:20])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded: 6180082\n","Example: ['Manuel Scorza', 'Manuel Scorza (Lima, 9 de setembro de 1928 - Madrid, 27 de novembro em 1983) foi um romancista e poeta Peruano da geração dos anos 50, pertencente ao Indigenismo ou Neoindigenismo peruano em conjunto com seus companheiros Ciro Alegría e José María Arguedas.', 'Scorza nasceu em 1928 de pai camponês e mãe índia. Mestiço, como quarenta e cinco por cento da população peruana, passou toda sua infância em Acoria (Huancavelica), um vilarejo dos Andes centrais. Ele completou seus estudos na Colégio Militar Leoncio Prado, que também estudaram os escritores peruanos Mario Vargas Llosa e Herbert Morote Rebolledo, dentre outros. Após os primeiros estudos em escolas públicas, obteve uma bolsa que lhe permitiu retornar para Lima, local de nascimento. Em 1945 entrou para a Universidade Nacional Mayor de San Marcos e iniciou um período febril de atividade política.', 'Scorza escrevia poemas desde os 16 anos, e pertencia à redação oposicionista em 1948, quando aconteceu a implementação do ditadura de Manuel Odría. Embora sem ter assinado artigos políticos, Scorza foi preso por um ano e, aos 20 anos, Scorza foi forçado, após o golpe de general Odría, a deixar o país como exilado para o México, onde viveu por 7 anos. Lá ele completou seus estudos em literatura numa universidade local. Ele viveu no Chile e no Brasil e finalmente se estabeleceu em Paris, França, onde aprendeu francês e conseguiu um trabalho de um certo prestígio leitor de espanhol na Escola Normal Superior de Saint-Cloud.', 'Muitos dos versos que compõem o seu primeiro livro,\"As maldições\"( 1955), são o resultado do desespero em que estava imerso. Ele não voltou para o Peru até o fim da ditadura, 10 anos mais tarde. No entanto, foi em sua narrativa, a partir do qual Alejo Carpentier foi um dos seus professores, onde Scorza encontrou o lugar ideal para residir sobre os problemas sociais do Peru. Na origem de sua carreira literária está uma revolta camponesa de 1960, ocorrida nos Andes centrais do Peru, envolvendo interesses econômicos de uma companhia mineira norte-americana, que tentava expulsar os lavradores de suas terras.', 'Em 1981 ele foi o primeiro em uma lista de escritores de renome internacional que o jornal \"Il Mattino\" convidou-o para ir a Nápoles para escrever uma série de artigos sobre uma cidade que depois de um segundo terremoto havia retornado ligeiro ressurgimento em 1980. Em 1983, depois de ter lançado em fevereiro o seu mais recente romance, \"\"A dança imóvel\"\", o Boeing 747 da Avianca voo 11 que iniciou a viagem para Bogotá, Scorza juntamente com outros intelectuais indo participar de uma conferência destinada a fazer um balanço da cultura latino-americana, caiu em uma colina sobre a abordagem ao aeroporto de Madrid. Manuel Scorza faleceu em 27 de novembro de 1983, aos 54 anos.', 'As \"\"Baladas\"\" (ou \"\"Cantares\"\") compõe-se de cinco novelas onde são descritas as lutas do campesinato peruano dos Andes Centrais que eram simplesmente comunidades massacradas isoladamente, pelo que cada uma enfrentava sozinha o poder.', 'Esta série de romances, ainda que cada novela constitua uma unidade independente, se sucedem numa sequência que as tornam partes de um todo.', 'Todas as cinco guerras silenciosas apresentam-se com conteúdo de fantasia poética e uma denúncia política e social. Elas foram traduzida para mais de 40 países.', '\"Baladas\" é um dos mais reconhecidos trabalhos da literatura peruana.', 'A primeira balada, \"Bom dia para os defuntos\" e \"Garabombo, o invisível\" (segunda balada) relatam os primórdios da luta, que precedem ao \"Cavaleiro Insone\" (Balada 3) onde se que descreve como foi que depois do massacre de Chinche, esses povos de Cerro de Pasco voltaram a se organizar para a grande batalha. As Baladas se completam com a publicação de \"Cantar de Agapito Robles\" (Balada 4) e \"A Tumba do Relâmpago\" (Balada 5) escrito entre 1977 e 1978, que fecha o ciclo de romances da \"guerra silenciosa\" (\"As Baladas\") contando o final da história.', 'As Baladas aliam-se com uma visão do mundo profundamente humanista voltada a seu povo e, antes de mais nada, para a grande tarefa de uma luta nacional contra o latifundio e o neocolonealismo dos grandes cartéis.', 'Never Too Far', '\"Never Too Far\" é uma \"canção\" gravada pela artista musical estadunidense Mariah Carey para sua primeira trilha sonora e oitavo álbum de estúdio, \"Glitter\" (2001). Foi escrito e produzido por ela e Jimmy Jam e Terry Lewis. A música foi lançada como o segundo \"single\" do álbum em 23 de outubro de 2001, pela Virgin Records. A música é uma balada de ritmo médio que liricamente lida com o coração partido. \"Never Too Far\" foi usado no \"single\" de caridade \"Never Too Far/Hero Medley\", que combina o primeiro verso da música com uma versão regravada do primeiro verso e a ponte do \"single\" anterior de Carey \"Hero\" (1993) .', 'O single não teve muito impacto nas paradas americanas; no entanto, alcançou o top 40 no Reino Unido e na Austrália, como parte de um lado A duplo com \"Don\\'t Stop (Funkin\\' 4 Jamaica)\". Uma edição de rádio de \"Never Too Far\" é lançada e encontrada como a faixa de abertura do lançamento da música. Carey não conseguiu filmar um videoclipe para o \"single\", pois estava se recuperando de um problema de saúde. Em vez disso, um vídeo foi criado usando uma cena tirada diretamente do filme \"Glitter\", onde Billie Frank (interpretada por Carey) canta a música no Madison Square Garden durante um show. Carey promoveu o medley \"Never Too Far/Hero\" através de apresentações ao vivo no Radio Music Awards de 2001, e no seu especial de TV \"A Home For The Holidays With Mariah Carey\" e 18 anos depois em sua \"Caution World Tour\".', 'Em abril de 2001, Carey assinou um contrato de gravação de US$ 100 milhões com a Virgin Records (EMI Records). Após a liberação de \"Glitter\" — seu primeiro álbum sob o novo selo — e o filme de mesmo nome, Carey embarcou em uma curta campanha promocional de promoção ao projeto. Em 19 de julho de 2001, Carey fez uma aparição surpresa no programa \"Total Request Live\" (\"TRL\") da MTV. Ela saiu do estúdio de gravação do programa, empurrando um carrinho de sorvete enquanto usava uma camisa grande demais. Aparentemente ansiosa e excessivamente entusiasmada, Carey começou a distribuir barras de sorvete individuais para fãs e convidados do programa, enquanto acenava para a multidão lá embaixo na Times Square, enquanto se divergia em um monólogo divagante sobre terapia. Carey então caminhou até a plataforma de Daly e começou um striptease, na qual ela tirou a camisa para revelar um conjunto amarelo e verde apertado, levando-o ao apresentador exclamar; \"Mariah Carey perdeu a cabeça!\".', 'Após outras aparições nas quais sua publicitária Cindy Berger disse que a cantora \"não estava pensando claramente\", em 26 de julho, ela foi hospitalizada, alegando \"exaustão extrema\" e \"colapso físico e emocional\". Após sua indução em um hospital em Connecticut, Carey permaneceu internada e sob cuidados do médico, durante duas semanas, seguido de uma ausência prolongada de promoção do álbum. No entanto, sua gravadora começou a promover \"Never Too Far\" como o segundo \"single\" da trilha sonora, mas Carey não foi capaz de promovê-lo devido à recuperação de seu colapso. A música foi lançada em 23 de outubro de 2001.', '\"Never Too Far\" é uma balada de tempo médio. A música foi escrita e produzida por Carey, Jimmy Jam e Terry Lewis; foi lançado como o segundo \"single\" de seu primeiro álbum de trilha sonora, \"Glitter\" (2001). Incorporando a sonoridade de vários instrumentos musicais, incluindo violão, piano e órgão. De acordo com as partituras publicadas no Musicnotes.com por Alfred Music Publishing, a música é definida em tempo comum com um ritmo moderado de 60 batidas por minuto. É composto na clave de Dó maior com alcance vocal de Carey que vai da nota baixa de D até a nota alta de Ab. O refrão da música tem uma progressão básica de acordes G–G–G/F–Em–G/D–C-G/B–Am. Descrita como uma \"canção de amor contemporânea e de ritmo lento\", a letra da música dizia \"Muito doloroso para falar sobre isso, então eu a seguro / então meu coração pode se consertar e ser corajoso o suficiente para amar de novo\", falando de emoções sentida pelo protagonista do filme. \"Never Too Far\" apresenta \"uma cama de cordas sintetizadas, bateria suave e violão ao estilo espanhol\" como sua instrumentação principal e incorpora notas de violino e teclado antes do primeiro verso. De acordo com Chuck Taylor, da \"Billboard\", Carey canta a letra com \"sutileza apreciável, deslizando sem esforço\" pela música. Termina com uma nota de 15 segundos que \"evoca um suspiro satisfeito\".', 'Após o fraco sucesso comercial de \"Glitter\", a outra gravadora de Carey, a Columbia Records, planejava lançar um álbum de compilação de seus maiores sucessos, intitulado \"Greatest Hits\" (2001). Como eles tinham mais um álbum para lançar de Carey sob seu antigo contrato, eles começaram a montar o conteúdo para seu lançamento. Embora ainda contratada da Virgin, Carey compôs um \"single\" de caridade no qual todos os recursos seriam destinados a ajudar a reconstruir os Estados Unidos, após os ataques terroristas de 11 de setembro de 2001. Consequentemente, ela regravou sua música de 1993 \"Hero\", e fez uma mistura com \"Never Too Far\". A música apresenta uma introdução instrumental diferente, e começa com o primeiro verso e coro de \"Never Too Far\" e se mistura à ponte de \"Hero\". Vários escritores do medley são creditados, com Carey tendo escrito \"Never Too Far\" com Jimmy Jam e Terry Lewis, e este último com Walter Afanasieff. Além de Jam e Lewis, Randy Jackson ajudou Carey na produção da música. Em uma entrevista à MTV, Carey descreveu o \"single\" e sua concepção:', \"Comecei a fazer diferentes eventos de caridade, onde fiz uma combinação de 'Never Too Far' e 'Hero'. Nós fizemos isso em um medley e colocamos na mesma clave e fizemos funcionar. As pessoas responderam muito bem a isso. Tem sido interessante para mim, desde os eventos de 11 de setembro, a maneira como as pessoas cantam 'Hero' e até mesmo conversam comigo sobre 'Never Too Far', porque essa música também é sobre perda. Imaginei que seria uma boa coisa fazer isso para colocar os dois no Natal. Há também uma faixa inédita no lado B, chamada 'There for Me', que meio que tem o mesmo sentimento. Os recursos de 'Never Too Far' e 'Hero' e 'There for Me' serão direcionados ao Fundo dos Heróis e beneficiarão as famílias dos policiais, famílias de trabalhadores humanitários.\"]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vZI4rXi0LwDF","colab_type":"text"},"source":["### Save in txt Format\n","\n","After pre processing, in order to train a tokenizer, we should dump the sentences to one or more txt files, so the tokenizer can be trained from it.\n","\n","An example of traing the Huggingface BERT WordPiece token can be found in https://github.com/huggingface/tokenizers/blob/master/bindings/python/examples/train_bert_wordpiece.py.\n","\n","The cached file is about 2GB, so every sentence occupy about 0,3 Kb. I decided to split this corpus in txt files each of the size of 300 Mb. So, each file will have approximately 900,000 sentences."]},{"cell_type":"code","metadata":{"id":"-iqEAJqoMHfp","colab_type":"code","colab":{}},"source":["import math\n","import shutil\n","\n","\n","SENTENCES_BY_FILE = 900000\n","TXT_LOCATION = '/content/wiki_pt'\n","\n","\n","def save_txt_corpus(sentences):\n","    \"\"\" \n","    Saves all pre processeded sentences in txt files.\n","    Every file contains up to 900k sentences and sentences are split into '\\n'.\n","    \"\"\"\n","    total_files = math.ceil(len(sentences) / SENTENCES_BY_FILE)\n","    sentences_per_file = {}\n","\n","    if not os.path.exists(TXT_LOCATION):\n","        os.mkdir(TXT_LOCATION)\n","\n","    for i in range(total_files):\n","        start_pos = i * SENTENCES_BY_FILE\n","        file_sentences = sentences[start_pos:start_pos + SENTENCES_BY_FILE]\n","        file_name = f'{TXT_LOCATION}/wiki_pt_{i}.txt'\n","\n","        sentences_per_file[file_name] = len(file_sentences)\n","\n","        with open(file_name, 'w+') as txt:\n","            txt.write('\\n'.join(file_sentences))\n","\n","    return sentences_per_file\n","\n","\n","def move_txt_to_drive(\n","    dest_location='/content/drive/My Drive/PF13/text_preprocessed/wiki_pt'):\n","    \"\"\" Move local txt corpus to Drive. \"\"\"\n","    shutil.copytree(TXT_LOCATION, dest_location)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0a58Uu4VPF7w","colab_type":"code","colab":{}},"source":["sentences_saved = save_txt_corpus(all_sentences)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0-7RmhpTnId","colab_type":"code","outputId":"e4a04fcc-b408-4b71-fe5d-609a2a30000c","executionInfo":{"status":"ok","timestamp":1590516166351,"user_tz":180,"elapsed":69031,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["sentences_saved"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'/content/wiki_pt/wiki_pt_0.txt': 900000,\n"," '/content/wiki_pt/wiki_pt_1.txt': 900000,\n"," '/content/wiki_pt/wiki_pt_2.txt': 900000,\n"," '/content/wiki_pt/wiki_pt_3.txt': 900000,\n"," '/content/wiki_pt/wiki_pt_4.txt': 900000,\n"," '/content/wiki_pt/wiki_pt_5.txt': 900000,\n"," '/content/wiki_pt/wiki_pt_6.txt': 780082}"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"hh6Np1IpUBYR","colab_type":"code","outputId":"6748f4dc-39a6-4670-b19c-48dbbb853785","executionInfo":{"status":"ok","timestamp":1590516166352,"user_tz":180,"elapsed":69019,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sum([v for i, v in sentences_saved.items()])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6180082"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"HtxFnz-cWG0W","colab_type":"code","colab":{}},"source":["# move_txt_to_drive()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAw5IBdpPQmN","colab_type":"code","outputId":"e16af493-3eee-4ffb-ddb7-476a40df64ba","executionInfo":{"status":"ok","timestamp":1590516170181,"user_tz":180,"elapsed":72827,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":214}},"source":["!tail /content/wiki_pt/wiki_pt_2.txt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["A ligação entre as prática destas duas artes marciais chinesas internas, \"xingyiquan\" e \"baguazhang\", ocorreu a partir das reuniões de Cheng Tinghua com seus amigos Li Tsun I, Chang Chao Tung, Liu Te Kuan, e Liu Wai Hsiang (aluno de \"Hsing-I\" de Chang Chao Tung).\n","Os encontros tinham como finalidade comparar seus estilos de luta e compartilhar suas descobertas, num ambiente de aprendizado mútuo.\n","Cheng Tinghua foi morto durante o Levante dos boxers, em 1900, quando os \"oito exércitos estrangeiros\" invadiram Pequim.\n","Um grupo de soldados alemães estava recrutando à força passantes locais para um trabalho a ser realizado perto da porta \"Chung Wen\", local onde Cheng tinha sua loja.\n","Ele foi detido pelos soldados, que tentaram alinhá-lo aos demais recrutas.\n","Cheng resistiu e tentou lutar, derrubando alguns dos seus algozes.\n","Ao tentar escapar saltando um muro, foi atingido por um disparo dos soldados.\n","Cheng Yulung (seu filho mais velho, 1875-1928), Cheng Youxin (segundo filho), Cheng Yougong, Feng Junyi, Gao Kexing, Gao Yisheng (1866-1951), Geng Jishan, Guo Tongde, Han Qiying, Hon Mu Xi, Kan Lingfeng, Li Cunyi, Li Hanzhang, Li Wenbiao, Liu Bin, Liu Zhenzong, Qin Cheng, Sun Lutang (1861-1932), Liu Dekuan, Yang Mingshan, Zhang Changfa, Zhang Yongde, Zhang Yukui, Zhou Yu Xiang, Zhang Zhao Dong (1859-1940)\n","Teorias de gauge na rede\n","Em física, teoria de gauge na rede é o estudo de teorias de gauge em um espaço-tempo discreto numa rede. Embora a maioria das teorias de gauge não sejam exatamente solúveis, são de grande utilidade pois podem ser estudadas por simulações computacionais. Espera-se que, executando simulações em rede progressivamente maiores, o comportamento da teoria correspondente no contínuo seja recuperado."],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tf8RLyz1lpp4","colab_type":"text"},"source":["# BERT Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"oi8qmej0RP9-","colab_type":"text"},"source":["Now we train our BERT Tokenizer, using BERT WordPiece. We follow the same rationale as Artetxe, Ruder and Yogatama (2020), using the same vocabulary size as the model we'll use in English language, not performing and normalization or lowercasing.\n"]},{"cell_type":"code","metadata":{"id":"KfwmuXIklrpP","colab_type":"code","colab":{}},"source":["from tokenizers import BertWordPieceTokenizer\n","from transformers import BertTokenizer, BertModel\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8c3-m0bqRfF5","colab_type":"text"},"source":["We check the parameters of our target tokenizer and model: bert-base-cased."]},{"cell_type":"code","metadata":{"id":"NjOrUgtlRk3p","colab_type":"code","outputId":"5468952d-6c86-4977-ffb9-511b32a3ccf7","executionInfo":{"status":"ok","timestamp":1590517485011,"user_tz":180,"elapsed":4146,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":372}},"source":["bert_base_cased = BertTokenizer.from_pretrained('bert-base-cased')\n","bert_model = BertModel.from_pretrained('bert-base-cased')\n","\n","print('=' * 50)\n","print('Tokenizer')\n","print('=' * 50)\n","print('Vocab Size=', bert_base_cased.vocab_size)\n","print('Max Length=', bert_base_cased.max_len)\n","\n","print('Max Length Sentence Pairs=', bert_base_cased.max_len_sentences_pair)\n","print('Special Tokens=', bert_base_cased.special_tokens_map)\n","print('Initial Config=', bert_base_cased.pretrained_init_configuration['bert-base-cased'])\n","print()\n","print('[SEP]:', bert_base_cased.sep_token_id)\n","print('[PAD]:', bert_base_cased.pad_token_id)\n","print('[UNK]:', bert_base_cased.unk_token_id)\n","print('[MASK]:', bert_base_cased.mask_token_id)\n","print('[CLS]:', bert_base_cased.cls_token_id)\n","print('=' * 50)\n","\n","print('BERT')\n","print('=' * 50)\n","print('Vocab Size=', bert_model.config.vocab_size)\n","print('Max Position Embeddings=', bert_model.config.max_position_embeddings)\n","print('=' * 50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==================================================\n","Tokenizer\n","==================================================\n","Vocab Size= 28996\n","Max Length= 512\n","Max Length Sentence Pairs= 509\n","Special Tokens= {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n","Initial Config= {'do_lower_case': False}\n","\n","[SEP]: 102\n","[PAD]: 0\n","[UNK]: 100\n","[MASK]: 103\n","[CLS]: 101\n","==================================================\n","BERT\n","==================================================\n","Vocab Size= 28996\n","Max Position Embeddings= 512\n","==================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r9XGKHpLKmi5","colab_type":"text"},"source":["We create an instance of `BertWordPieceTokenizer`, the same algorithm used in the pre trained tokenizer from `transformers` library. We do not perform lowercasing. However, we keep accents (which are common in Portuguese) and handle Chinese Characters, in case one appears frequently in the corpus."]},{"cell_type":"code","metadata":{"id":"kMPDcsJGW4kt","colab_type":"code","colab":{}},"source":["pt_tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False,\n","                                      handle_chinese_chars=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-0DQw0HQMXPe","colab_type":"text"},"source":["We'll train the tokenizer in the following files:"]},{"cell_type":"code","metadata":{"id":"ux_AkIyjMayF","colab_type":"code","outputId":"8cbdd6b6-08c4-4003-9103-1251ecf604bf","executionInfo":{"status":"ok","timestamp":1590516889174,"user_tz":180,"elapsed":590,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["all_files = [\n","             os.path.join(TXT_LOCATION, txt) for txt\n","             in os.listdir(TXT_LOCATION)\n","             if txt.endswith('.txt')\n","            ]\n","all_files"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/wiki_pt/wiki_pt_2.txt',\n"," '/content/wiki_pt/wiki_pt_3.txt',\n"," '/content/wiki_pt/wiki_pt_6.txt',\n"," '/content/wiki_pt/wiki_pt_1.txt',\n"," '/content/wiki_pt/wiki_pt_4.txt',\n"," '/content/wiki_pt/wiki_pt_5.txt',\n"," '/content/wiki_pt/wiki_pt_0.txt']"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"-hhW_5d3cotv","colab_type":"text"},"source":["First, we do some plumbing in order to be compatible with pre trained BERT from transformers. The pre-trained tokenizer has some unused tokens at first, in order to keep the special tokens in certain positions.\n","\n","As I'll reuse the BERT special tokens based on Artetxe, Ruder and Yogatama (2020), I add the following tokens to preserve the ids. "]},{"cell_type":"code","metadata":{"id":"_-Dzp8ZdXGcd","colab_type":"code","colab":{}},"source":["initial_ =  ['[PAD]'] + \\\n","            [f'[unused{i}]' for i in range(1, 100)] + \\\n","            ['[UNK]', '[CLS]', '[SEP]', '[MASK]'] + \\\n","            ['[unused100]', '[unused101]']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8fBL4RVAMcS0","colab_type":"text"},"source":["We save the vocab for being able to use it later. We name our model `bert-base-cased-pt`."]},{"cell_type":"code","metadata":{"id":"npWzTMuTmINZ","colab_type":"code","colab":{}},"source":["pt_tokenizer.train(\n","    files=all_files, special_tokens=initial_,\n","    vocab_size=bert_base_cased.vocab_size) # Using the same vocab size\n","\n","tokenizer_files = pt_tokenizer.save('/content/', 'bert-base-cased-pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0KvwXw7ZjtP","colab_type":"code","outputId":"ec408de1-9b47-456a-f4b7-3c2f3a8e93e9","executionInfo":{"status":"ok","timestamp":1590521066256,"user_tz":180,"elapsed":614,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# for tokenizer_file in tokenizer_files:\n","#     print('Copying', tokenizer_file)\n","#     shutil.copy(tokenizer_file, '/content/drive/My Drive/PF13')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Copying /content/bert-base-cased-pt-vocab.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vPV1Pdr4asLK","colab_type":"text"},"source":["## Test Pre Trained Vocabulary"]},{"cell_type":"code","metadata":{"id":"nYUiLIYCau-J","colab_type":"code","outputId":"f36cc099-1d8a-46a7-d04e-87a5207dc031","executionInfo":{"status":"ok","timestamp":1590521106266,"user_tz":180,"elapsed":587,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":194}},"source":["from itertools import islice\n","\n","VOCAB_LOCATION='/content/drive/My Drive/PF13/bert-base-cased-pt-vocab.txt'\n","\n","pt_bert_tokenizer = BertTokenizer(VOCAB_LOCATION, do_lower_case=False,\n","                                  model_max_length=512)\n","\n","list(islice(pt_bert_tokenizer.vocab.items(), 10))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('[PAD]', 0),\n"," ('[unused1]', 1),\n"," ('[unused2]', 2),\n"," ('[unused3]', 3),\n"," ('[unused4]', 4),\n"," ('[unused5]', 5),\n"," ('[unused6]', 6),\n"," ('[unused7]', 7),\n"," ('[unused8]', 8),\n"," ('[unused9]', 9)]"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"KHlnXzaYa4ZX","colab_type":"code","outputId":"56f3c082-7ea7-4970-bf74-723b15196f18","executionInfo":{"status":"ok","timestamp":1590521113365,"user_tz":180,"elapsed":764,"user":{"displayName":"Leandro Rodrigues de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2FC6cRscIqjidLyBHUsWxxlbYXJYoPwiQM2tk=s64","userId":"13608136132232646902"}},"colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["print('=' * 50)\n","print('Tokenizer')\n","print('=' * 50)\n","print('Vocab Size=', pt_bert_tokenizer.vocab_size)\n","print('Max Length=', pt_bert_tokenizer.max_len)\n","\n","print('Max Length Sentence Pairs=', pt_bert_tokenizer.max_len_sentences_pair)\n","print('Special Tokens=', pt_bert_tokenizer.special_tokens_map)\n","print()\n","print('[SEP]:', pt_bert_tokenizer.sep_token_id)\n","print('[PAD]:', pt_bert_tokenizer.pad_token_id)\n","print('[UNK]:', pt_bert_tokenizer.unk_token_id)\n","print('[MASK]:', pt_bert_tokenizer.mask_token_id)\n","print('[CLS]:', pt_bert_tokenizer.cls_token_id)\n","print('=' * 50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==================================================\n","Tokenizer\n","==================================================\n","Vocab Size= 28996\n","Max Length= 512\n","Max Length Sentence Pairs= 509\n","Special Tokens= {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n","\n","[SEP]: 102\n","[PAD]: 0\n","[UNK]: 100\n","[MASK]: 103\n","[CLS]: 101\n","==================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oL1mQpZTCUo4","colab_type":"text"},"source":["# References\n","\n","Artetxe, Mikel, Sebastian Ruder, and Dani Yogatama. \"On the cross-lingual transferability of monolingual representations.\" arXiv preprint arXiv:1910.11856 (2020).\n"]},{"cell_type":"code","metadata":{"id":"C6nNxCdwOVx7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}